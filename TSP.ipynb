{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2da4c57-adb6-4f50-8778-5bb2806e1185",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TSP Factor-Graph Message-Passing Solver\n",
    "========================================\n",
    "논문의 알고리즘을 식 번호 단위로 정확히 구현.\n",
    "\n",
    "구조:\n",
    "  1) Trellis Module   — Forward ψ,α (eq 15-16) / Backward β,ξ (eq 17-18)\n",
    "  2) Assignment Module — λ_it (eq 21-27), ζ_it (eq 19-20/23)\n",
    "  3) Bipartite Matching Module — γ̃,ω̃,φ̃,η̃,ρ̃,δ̃ (eq 37-42)\n",
    "\n",
    "상태 표현:\n",
    "  x_t = (m_t, a_t)\n",
    "    m_t ∈ {0,1}^N : 방문 마스크 (N-bit)\n",
    "    a_t ∈ {0,...,N-1} : 현재 방문 노드\n",
    "\n",
    "시간: t = 1,...,T (T = N), depot에서 출발/귀환.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "NEG = -1e12  # numerical -∞\n",
    "\n",
    "\n",
    "class TSPFactorGraphSolver:\n",
    "\n",
    "    def __init__(self, D, start_city=0,\n",
    "                 damping=0.3, iters=200, verbose=False,\n",
    "                 seed=0, patience=20, cost_tol=1e-12):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        D : (C x C) 거리 행렬. C = N+1 (N개 도시 + 1개 depot).\n",
    "        start_city : depot으로 사용할 노드 인덱스.\n",
    "        damping : γ̃, ω̃ 업데이트 시 damping factor (0~1).\n",
    "        iters : 최대 반복 횟수.\n",
    "        verbose : 매 반복 출력 여부.\n",
    "        patience : cost 변화 없으면 조기 종료까지의 횟수.\n",
    "        cost_tol : cost 변화 판단 임계값.\n",
    "        \"\"\"\n",
    "        D = np.array(D, dtype=float)\n",
    "        assert D.shape[0] == D.shape[1], \"D must be square\"\n",
    "        C = D.shape[0]\n",
    "        N = C - 1\n",
    "\n",
    "        # --- 내부 인덱싱: start_city를 마지막 인덱스(depot)로 이동 ---\n",
    "        perm = np.arange(C)\n",
    "        if start_city != C - 1:\n",
    "            perm[start_city], perm[C - 1] = perm[C - 1], perm[start_city]\n",
    "        inv_perm = np.empty(C, dtype=int)\n",
    "        inv_perm[perm] = np.arange(C)\n",
    "\n",
    "        self.orig_D = D\n",
    "        self.D_perm = D[perm][:, perm]\n",
    "        self.perm = perm\n",
    "        self.inv_perm = inv_perm\n",
    "        self.C = C\n",
    "        self.N = N               # 도시 수 (depot 제외)\n",
    "        self.depot = C - 1       # 내부 depot 인덱스\n",
    "\n",
    "        # --- Similarity matrix (eq 5): s(u,v) = max(D) - D(u,v) ---\n",
    "        mx = np.max(self.D_perm)\n",
    "        self.s = mx - self.D_perm\n",
    "\n",
    "        self.T = N                # 시간 단계 수\n",
    "        self.M = 1 << N           # 마스크 상태 수\n",
    "\n",
    "        self.damping = float(damping)\n",
    "        self.iters = int(iters)\n",
    "        self.verbose = bool(verbose)\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.patience = int(patience)\n",
    "        self.cost_tol = float(cost_tol)\n",
    "\n",
    "        # --- Bipartite Matching 메시지 초기화 (shape: [N, T]) ---\n",
    "        # γ̃_it, ω̃_it 만 상태로 유지 (damping 적용 대상)\n",
    "        self.gamma_t = np.zeros((N, N))   # γ̃_it\n",
    "        self.omega_t = np.zeros((N, N))   # ω̃_it\n",
    "\n",
    "    # =================================================================\n",
    "    #                         Public Interface\n",
    "    # =================================================================\n",
    "    def run(self):\n",
    "        \"\"\"메시지 패싱 반복 실행. (best_route, best_cost) 반환.\"\"\"\n",
    "        best_route, best_cost = None, None\n",
    "        stable = 0\n",
    "        last_cost = None\n",
    "\n",
    "        for it in range(self.iters):\n",
    "            # ── (1) BM 메시지에서 φ̃, η̃, ρ̃ 도출 (eq 37, 40, 41) ──\n",
    "            phi_t, eta_t, rho_t = self._derive_bm_messages()\n",
    "\n",
    "            # ── (2) Forward pass: ψ_t, α_t (eq 15-16) ──\n",
    "            psi, alpha, backptr = self._forward(rho_t)\n",
    "\n",
    "            # ── (3) Backward pass: β_t, ξ_t (eq 17-18) ──\n",
    "            beta, xi = self._backward(rho_t)\n",
    "\n",
    "            # ── (4) δ̃_it 계산 (eq 42, via ζ from eq 23) ──\n",
    "            delta_t = self._compute_delta(psi, beta, rho_t)\n",
    "\n",
    "            # ── (5) γ̃, ω̃ 업데이트 with damping (eq 38-39) ──\n",
    "            gamma_new = eta_t + delta_t    # eq (39): γ̃ = η̃ + δ̃\n",
    "            omega_new = phi_t + delta_t    # eq (38): ω̃ = φ̃ + δ̃\n",
    "            self.gamma_t = self.damping * gamma_new + (1 - self.damping) * self.gamma_t\n",
    "            self.omega_t = self.damping * omega_new + (1 - self.damping) * self.omega_t\n",
    "\n",
    "            # ── (6) Decode & cost ──\n",
    "            route = self._decode(alpha, backptr)\n",
    "            cost = self._route_cost(route)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"[{it+1:03d}] cost={cost:.6f}  route={route}\")\n",
    "\n",
    "            if best_cost is None or cost < best_cost:\n",
    "                best_cost, best_route = cost, route\n",
    "\n",
    "            # Early stopping\n",
    "            if last_cost is not None and abs(cost - last_cost) <= self.cost_tol:\n",
    "                stable += 1\n",
    "            else:\n",
    "                stable = 0\n",
    "            last_cost = cost\n",
    "            if stable >= self.patience:\n",
    "                break\n",
    "\n",
    "        return best_route, best_cost\n",
    "\n",
    "    # =================================================================\n",
    "    #        Bipartite Matching: φ̃, η̃, ρ̃ 도출 (eq 37, 40, 41)\n",
    "    # =================================================================\n",
    "    def _derive_bm_messages(self):\n",
    "        \"\"\"\n",
    "        현재 γ̃, ω̃로부터:\n",
    "          φ̃_it = -max_{i'≠i} γ̃_{i't}   ... (40)\n",
    "          η̃_it = -max_{t'≠t} ω̃_{it'}   ... (37)\n",
    "          ρ̃_it = η̃_it + φ̃_it           ... (41)\n",
    "        \"\"\"\n",
    "        N, T = self.N, self.T\n",
    "        phi_t = np.zeros((N, T))\n",
    "        eta_t = np.zeros((N, T))\n",
    "\n",
    "        # eq (40): φ̃_it = -max_{i'≠i} γ̃_{i't}\n",
    "        for t in range(T):\n",
    "            col = self.gamma_t[:, t]\n",
    "            for i in range(N):\n",
    "                phi_t[i, t] = -np.max(np.delete(col, i)) if N > 1 else 0.0\n",
    "\n",
    "        # eq (37): η̃_it = -max_{t'≠t} ω̃_{it'}\n",
    "        for i in range(N):\n",
    "            row = self.omega_t[i, :]\n",
    "            for t in range(T):\n",
    "                eta_t[i, t] = -np.max(np.delete(row, t)) if T > 1 else 0.0\n",
    "\n",
    "        # eq (41): ρ̃_it = η̃_it + φ̃_it\n",
    "        rho_t = eta_t + phi_t\n",
    "\n",
    "        return phi_t, eta_t, rho_t\n",
    "\n",
    "    # =================================================================\n",
    "    #             Assignment Module: λ 관련 헬퍼 (eq 24-27)\n",
    "    # =================================================================\n",
    "    def _lambda_sum(self, rho_t, t_idx, a):\n",
    "        \"\"\"\n",
    "        Σ_i λ_it(a_t=a).\n",
    "\n",
    "        Row-gauge-fix (eq 24-27)에 의해:\n",
    "          λ_it(a) = (N-1)/N · ρ̃_it  if a=i\n",
    "                   -1/N    · ρ̃_it  if a≠i\n",
    "\n",
    "        합산하면:\n",
    "          Σ_i λ_it(a) = ρ̃_{a,t} - (1/N) Σ_j ρ̃_{j,t}\n",
    "        \"\"\"\n",
    "        return rho_t[a, t_idx] - np.mean(rho_t[:, t_idx])\n",
    "\n",
    "    def _lambda_it_val(self, rho_t, i, t_idx, a):\n",
    "        \"\"\"\n",
    "        λ_it(x_t) where a_t = a.  (eq 27)\n",
    "          = (N-1)/N · ρ̃_it   if a = i\n",
    "          = -1/N    · ρ̃_it   if a ≠ i\n",
    "        \"\"\"\n",
    "        N = self.N\n",
    "        if a == i:\n",
    "            return (N - 1.0) / N * rho_t[i, t_idx]\n",
    "        else:\n",
    "            return -1.0 / N * rho_t[i, t_idx]\n",
    "\n",
    "    def _lambda_sum_excl_i(self, rho_t, i, t_idx, a):\n",
    "        \"\"\"\n",
    "        Σ_{i'≠i} λ_{i't}(a_t=a)  =  Σ_i λ_it(a) - λ_it(a).\n",
    "\n",
    "        eq (19): ζ_it = ψ_t + β_t + Σ_{i'≠i} λ_{i't}  에서 사용.\n",
    "        \"\"\"\n",
    "        return self._lambda_sum(rho_t, t_idx, a) - self._lambda_it_val(rho_t, i, t_idx, a)\n",
    "\n",
    "    # =================================================================\n",
    "    #             Trellis Forward Pass: ψ_t, α_t (eq 15-16)\n",
    "    # =================================================================\n",
    "    def _forward(self, rho_t):\n",
    "        \"\"\"\n",
    "        eq (15): ψ_t(x_t) = max_{x_{t-1}} [ G_t(x_{t-1}, x_t) + α_{t-1}(x_{t-1}) ]\n",
    "        eq (16): α_t(x_t) = ψ_t(x_t) + Σ_i λ_it(x_t)\n",
    "\n",
    "        반환: psi[t, mask, a], alpha[t, mask, a], backptr[t, mask, a]\n",
    "        \"\"\"\n",
    "        T, N, M = self.T, self.N, self.M\n",
    "        depot = self.depot\n",
    "\n",
    "        psi   = np.full((T + 1, M, N), NEG)\n",
    "        alpha = np.full((T + 1, M, N), NEG)\n",
    "        backptr = np.full((T + 1, M, N, 2), -1, dtype=int)\n",
    "\n",
    "        # ── t = 1: depot → 첫 번째 도시 ──\n",
    "        # α_0(x_0) = 0  (depot 단일 상태)\n",
    "        # G_1(x_0, x_1) = s(depot, a_1)\n",
    "        # ψ_1({a}, a) = s(depot, a) + 0 = s(depot, a)\n",
    "        # α_1({a}, a) = ψ_1 + Σ_i λ_{i,1}(a)\n",
    "        for a in range(N):\n",
    "            m = 1 << a\n",
    "            psi[1, m, a] = self.s[depot, a]\n",
    "            alpha[1, m, a] = psi[1, m, a] + self._lambda_sum(rho_t, 0, a)\n",
    "            backptr[1, m, a] = (0, -1)  # sentinel\n",
    "\n",
    "        # ── t = 2 .. T ──\n",
    "        for t in range(2, T + 1):\n",
    "            t_idx = t - 1   # ρ̃의 0-based 시간 인덱스\n",
    "            for mask in range(M):\n",
    "                if bin(mask).count('1') != t:\n",
    "                    continue\n",
    "                for a in range(N):\n",
    "                    if not (mask & (1 << a)):\n",
    "                        continue  # a ∈ mask 이어야 유효\n",
    "                    prev_mask = mask ^ (1 << a)\n",
    "\n",
    "                    best = NEG\n",
    "                    best_last = -1\n",
    "                    # prev_mask 안의 모든 last에 대해 max\n",
    "                    m = prev_mask\n",
    "                    while m:\n",
    "                        last = (m & -m).bit_length() - 1\n",
    "                        m ^= (1 << last)\n",
    "                        # eq (15): G_t + α_{t-1}\n",
    "                        cand = alpha[t - 1, prev_mask, last] + self.s[last, a]\n",
    "                        if cand > best:\n",
    "                            best = cand\n",
    "                            best_last = last\n",
    "\n",
    "                    if best > NEG / 2:\n",
    "                        psi[t, mask, a] = best                                    # eq (15)\n",
    "                        alpha[t, mask, a] = best + self._lambda_sum(rho_t, t_idx, a)  # eq (16)\n",
    "                        backptr[t, mask, a] = (prev_mask, best_last)\n",
    "\n",
    "        return psi, alpha, backptr\n",
    "\n",
    "    # =================================================================\n",
    "    #            Trellis Backward Pass: β_t, ξ_t (eq 17-18)\n",
    "    # =================================================================\n",
    "    def _backward(self, rho_t):\n",
    "        \"\"\"\n",
    "        eq (18): β_t(x_t) = max_{x_{t+1}} [ G_{t+1}(x_t, x_{t+1}) + ξ_{t+1}(x_{t+1}) ]\n",
    "        eq (17): ξ_t(x_t) = β_t(x_t) + Σ_i λ_it(x_t)\n",
    "\n",
    "        경계: t = T → β_T(full, a) = s(a, depot)  (depot 귀환)\n",
    "\n",
    "        반환: beta[t, mask, a], xi[t, mask, a]\n",
    "        \"\"\"\n",
    "        T, N, M = self.T, self.N, self.M\n",
    "        full = (1 << N) - 1\n",
    "        depot = self.depot\n",
    "\n",
    "        beta = np.full((T + 1, M, N), NEG)\n",
    "        xi   = np.full((T + 1, M, N), NEG)\n",
    "\n",
    "        # ── t = T: closure (마지막 도시 → depot) ──\n",
    "        for a in range(N):\n",
    "            beta[T, full, a] = self.s[a, depot]\n",
    "            xi[T, full, a] = beta[T, full, a] + self._lambda_sum(rho_t, T - 1, a)\n",
    "\n",
    "        # ── t = T-1 .. 1 ──\n",
    "        for t in range(T - 1, 0, -1):\n",
    "            t_idx = t - 1   # ρ̃의 0-based 시간 인덱스\n",
    "            for mask in range(M):\n",
    "                if bin(mask).count('1') != t:\n",
    "                    continue\n",
    "                for last in range(N):\n",
    "                    if not (mask & (1 << last)):\n",
    "                        continue  # last ∈ mask\n",
    "\n",
    "                    best = NEG\n",
    "                    avail = (~mask) & full\n",
    "                    m = avail\n",
    "                    while m:\n",
    "                        a = (m & -m).bit_length() - 1\n",
    "                        m ^= (1 << a)\n",
    "                        new_mask = mask | (1 << a)\n",
    "                        # eq (18): G_{t+1} + ξ_{t+1}\n",
    "                        cand = self.s[last, a] + xi[t + 1, new_mask, a]\n",
    "                        if cand > best:\n",
    "                            best = cand\n",
    "\n",
    "                    if best > NEG / 2:\n",
    "                        beta[t, mask, last] = best                                      # eq (18)\n",
    "                        xi[t, mask, last] = best + self._lambda_sum(rho_t, t_idx, last)  # eq (17)\n",
    "\n",
    "        return beta, xi\n",
    "\n",
    "    # =================================================================\n",
    "    #                   δ̃_it 계산 (eq 42, via ζ eq 23)\n",
    "    # =================================================================\n",
    "    def _compute_delta(self, psi, beta, rho_t):\n",
    "        \"\"\"\n",
    "        eq (42):\n",
    "          δ̃_it = max_{m_t} ζ_it(m_t, i)  -  max_{m_t, a_t≠i} ζ_it(m_t, a_t)\n",
    "\n",
    "        eq (23):\n",
    "          ζ_it(x_t) = ψ_t(x_t) + β_t(x_t) + Σ_{i'≠i} λ_{i't}(a_t)\n",
    "\n",
    "        ψ + β = Γ (total path metric without λ at time t).\n",
    "        Σ_{i'≠i} λ 는 a_t에만 의존 (m_t에 무관).\n",
    "        \"\"\"\n",
    "        T, N, M = self.T, self.N, self.M\n",
    "        delta = np.zeros((N, T))\n",
    "\n",
    "        for t in range(1, T + 1):\n",
    "            t_idx = t - 1\n",
    "\n",
    "            for i in range(N):\n",
    "                best_with = NEG      # max over (mask, a_t=i)\n",
    "                best_without = NEG   # max over (mask, a_t≠i)\n",
    "\n",
    "                for mask in range(M):\n",
    "                    if bin(mask).count('1') != t:\n",
    "                        continue\n",
    "\n",
    "                    # ── a_t = i (bit=1 case) ──\n",
    "                    if mask & (1 << i):\n",
    "                        gamma_v = psi[t, mask, i] + beta[t, mask, i]\n",
    "                        if gamma_v > NEG / 2:\n",
    "                            z = gamma_v + self._lambda_sum_excl_i(rho_t, i, t_idx, i)\n",
    "                            if z > best_with:\n",
    "                                best_with = z\n",
    "\n",
    "                    # ── a_t ≠ i (bit=0 case) ──\n",
    "                    m2 = mask\n",
    "                    while m2:\n",
    "                        a = (m2 & -m2).bit_length() - 1\n",
    "                        m2 ^= (1 << a)\n",
    "                        if a == i:\n",
    "                            continue\n",
    "                        gamma_v = psi[t, mask, a] + beta[t, mask, a]\n",
    "                        if gamma_v > NEG / 2:\n",
    "                            z = gamma_v + self._lambda_sum_excl_i(rho_t, i, t_idx, a)\n",
    "                            if z > best_without:\n",
    "                                best_without = z\n",
    "\n",
    "                if best_with <= NEG / 2 and best_without <= NEG / 2:\n",
    "                    delta[i, t_idx] = 0.0\n",
    "                else:\n",
    "                    delta[i, t_idx] = best_with - best_without\n",
    "\n",
    "        return delta\n",
    "\n",
    "    # =================================================================\n",
    "    #                         Route Decoding\n",
    "    # =================================================================\n",
    "    def _decode(self, alpha, backptr):\n",
    "        \"\"\"\n",
    "        α_T(full, a) + s(a, depot)를 최대화하는 경로를 backtrack.\n",
    "        \"\"\"\n",
    "        T, N = self.T, self.N\n",
    "        full = (1 << N) - 1\n",
    "        depot = self.depot\n",
    "\n",
    "        best_val = NEG\n",
    "        best_last = -1\n",
    "        for a in range(N):\n",
    "            val = alpha[T, full, a] + self.s[a, depot]\n",
    "            if val > best_val:\n",
    "                best_val = val\n",
    "                best_last = a\n",
    "\n",
    "        if best_last < 0:\n",
    "            # Fallback: greedy nearest-neighbor\n",
    "            route = [depot]\n",
    "            used = set()\n",
    "            for _ in range(N):\n",
    "                scores = self.s[route[-1], :N].copy()\n",
    "                for u in used:\n",
    "                    scores[u] = NEG\n",
    "                a = int(np.argmax(scores))\n",
    "                used.add(a)\n",
    "                route.append(a)\n",
    "            route.append(depot)\n",
    "        else:\n",
    "            route_inner = []\n",
    "            mask = full\n",
    "            last = best_last\n",
    "            t = T\n",
    "            while t > 0 and last >= 0:\n",
    "                route_inner.append(last)\n",
    "                pm, pl = backptr[t, mask, last]\n",
    "                mask, last = pm, pl\n",
    "                t -= 1\n",
    "            route_inner.reverse()\n",
    "            route = [depot] + route_inner + [depot]\n",
    "\n",
    "        # 내부 인덱스 → 원래 인덱스 변환\n",
    "        return [int(self.inv_perm[c]) for c in route]\n",
    "\n",
    "    def _route_cost(self, route):\n",
    "        \"\"\"원래 거리 행렬 기준 경로 비용.\"\"\"\n",
    "        return float(sum(self.orig_D[route[k], route[k + 1]]\n",
    "                         for k in range(len(route) - 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "996f6e28-aa70-4e94-ae71-f1bbce831053",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TSP Factor-Graph Message-Passing Solver — GPU-Accelerated (PyTorch)\n",
    "====================================================================\n",
    "핵심 가속 전략:\n",
    "  1) 마스크별 순차 루프 제거 → 동일 popcount 마스크를 텐서 배치로 처리\n",
    "  2) Forward/Backward에서 per-city 루프 + 배치 gather/scatter\n",
    "  3) δ̃ 계산: peak 팩토리제이션으로 O(N²T·M) → O(N²T + N·T·M) 축소\n",
    "\n",
    "메모리 프로파일 (float64 기준):\n",
    "  psi, alpha, beta, xi : 각 [T+1, M, N] → N=15: ~61MB, N=20: ~320MB\n",
    "  중간 텐서 (per step)  : [M, N]         → N=15: ~4MB,  N=20: ~160MB\n",
    "  총 피크: N=15 ~300MB, N=20 ~2GB\n",
    "\n",
    "권장 N 범위: ≤ 20 (GPU 24GB 기준), ≤ 15 (GPU 8GB 기준)\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import gc\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "NEG = -1e12\n",
    "\n",
    "\n",
    "def get_device(device: Optional[str] = None) -> torch.device:\n",
    "    \"\"\"CUDA > MPS > CPU 자동 선택.\"\"\"\n",
    "    if device is not None:\n",
    "        return torch.device(device)\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def flush_gpu(device: Optional[torch.device] = None):\n",
    "    \"\"\"GPU 메모리 강제 정리. solver 생성 전/후에 호출 가능.\"\"\"\n",
    "    gc.collect()\n",
    "    if device is not None and device.type == \"cpu\":\n",
    "        return\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()          # 미완료 커널 대기\n",
    "        gc.collect()                       # Python 참조 해제\n",
    "        torch.cuda.empty_cache()           # PyTorch 캐시 → OS 반환\n",
    "        torch.cuda.ipc_collect()           # IPC 공유 메모리 정리\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        gc.collect()\n",
    "        if hasattr(torch.mps, \"empty_cache\"):\n",
    "            torch.mps.empty_cache()        # PyTorch 2.1+\n",
    "\n",
    "\n",
    "class TSPFactorGraphSolverGPU:\n",
    "\n",
    "    def __init__(self, D, start_city: int = 0,\n",
    "                 damping: float = 0.3, iters: int = 200,\n",
    "                 verbose: bool = False, seed: int = 0,\n",
    "                 patience: int = 20, cost_tol: float = 1e-12,\n",
    "                 device: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        D : (C x C) 거리 행렬. C = N+1 (N개 도시 + 1개 depot).\n",
    "        device : 'cuda', 'mps', 'cpu' 또는 None (자동).\n",
    "        \"\"\"\n",
    "        self.device = get_device(device)\n",
    "        self.dtype = torch.float32\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        # --- 기존 GPU 텐서 정리 ---\n",
    "        flush_gpu(self.device)\n",
    "\n",
    "        D_np = np.array(D, dtype=np.float32)\n",
    "        assert D_np.shape[0] == D_np.shape[1], \"D must be square\"\n",
    "        C = D_np.shape[0]\n",
    "        N = C - 1\n",
    "\n",
    "        # --- 내부 인덱싱: start_city → 마지막 인덱스(depot) ---\n",
    "        perm = np.arange(C)\n",
    "        if start_city != C - 1:\n",
    "            perm[start_city], perm[C - 1] = perm[C - 1], perm[start_city]\n",
    "        inv_perm = np.empty(C, dtype=int)\n",
    "        inv_perm[perm] = np.arange(C)\n",
    "\n",
    "        self.orig_D = torch.tensor(D_np, dtype=self.dtype, device=self.device)\n",
    "        D_perm = D_np[perm][:, perm]\n",
    "        self.D_perm = torch.tensor(D_perm, dtype=self.dtype, device=self.device)\n",
    "        self.inv_perm = inv_perm\n",
    "        self.C, self.N, self.depot = C, N, C - 1\n",
    "\n",
    "        # Similarity (eq 5)\n",
    "        self.s = self.D_perm.max() - self.D_perm\n",
    "        # 도시 간 유사도 서브매트릭스 [N, N]\n",
    "        self.s_city = self.s[:N, :N]\n",
    "\n",
    "        self.T = N\n",
    "        self.M = 1 << N\n",
    "        self.damping = damping\n",
    "        self.iters = iters\n",
    "        self.verbose = verbose\n",
    "        self.patience = patience\n",
    "        self.cost_tol = cost_tol\n",
    "\n",
    "        # BM 메시지\n",
    "        self.gamma_t = torch.zeros((N, N), dtype=self.dtype, device=self.device)\n",
    "        self.omega_t = torch.zeros((N, N), dtype=self.dtype, device=self.device)\n",
    "\n",
    "        # --- 마스크 테이블 사전 계산 ---\n",
    "        self._precompute_tables()\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"[Device: {self.device}] N={N}, M={self.M}, \"\n",
    "                  f\"peak mem ~{self._estimate_mem_mb():.0f} MB\")\n",
    "\n",
    "    # =================================================================\n",
    "    #                    마스크 테이블 사전 계산\n",
    "    # =================================================================\n",
    "    def _precompute_tables(self):\n",
    "        N, M = self.N, self.M\n",
    "        dev, dt = self.device, self.dtype\n",
    "\n",
    "        mask_range = torch.arange(M, device=dev, dtype=torch.long)\n",
    "\n",
    "        # popcount[m] = number of set bits in m\n",
    "        pc = torch.zeros(M, dtype=torch.int32, device=dev)\n",
    "        for i in range(N):\n",
    "            pc += ((mask_range >> i) & 1).int()\n",
    "        self._popcount = pc\n",
    "\n",
    "        # bit_set[m, a] = bool: bit a is set in mask m\n",
    "        bit_vals = (1 << torch.arange(N, device=dev, dtype=torch.long))\n",
    "        self._bit_set = (mask_range.unsqueeze(1) & bit_vals.unsqueeze(0)) != 0  # [M, N]\n",
    "\n",
    "        # prev_mask[m, a] = m ^ (1<<a)  (remove bit a)\n",
    "        self._prev_mask = mask_range.unsqueeze(1) ^ bit_vals.unsqueeze(0)  # [M, N]\n",
    "\n",
    "        # next_mask[m, a] = m | (1<<a)  (add bit a)\n",
    "        self._next_mask = mask_range.unsqueeze(1) | bit_vals.unsqueeze(0)  # [M, N]\n",
    "\n",
    "        # NEG 텐서 캐시\n",
    "        self._NEG_M = torch.full((M,), NEG, dtype=dt, device=dev)\n",
    "        self._NEG_MN = torch.full((M, N), NEG, dtype=dt, device=dev)\n",
    "        self._NEG_1 = torch.tensor(NEG, dtype=dt, device=dev)\n",
    "        self._ZERO = torch.tensor(0.0, dtype=dt, device=dev)\n",
    "        self._NEG1_long = torch.tensor(-1, dtype=torch.long, device=dev)\n",
    "\n",
    "    def _estimate_mem_mb(self):\n",
    "        T, M, N = self.T, self.M, self.N\n",
    "        main = 4 * (T + 1) * M * N * 8\n",
    "        tables = 3 * M * N * 8\n",
    "        return (main + tables) / (1024 ** 2)\n",
    "\n",
    "    # =================================================================\n",
    "    #                    GPU 메모리 관리\n",
    "    # =================================================================\n",
    "    def cleanup(self):\n",
    "        \"\"\"명시적 GPU 메모리 해제. 솔버 사용 후 호출 권장.\"\"\"\n",
    "        dev = self.device if hasattr(self, 'device') else None\n",
    "\n",
    "        # 모든 텐서 속성 삭제\n",
    "        tensor_attrs = [k for k, v in self.__dict__.items()\n",
    "                        if isinstance(v, torch.Tensor)]\n",
    "        for attr in tensor_attrs:\n",
    "            delattr(self, attr)\n",
    "\n",
    "        # 사전 계산 테이블 등 나머지 대형 속성\n",
    "        for attr in ['_bit_set', '_prev_mask', '_next_mask', '_popcount',\n",
    "                      '_NEG_M', '_NEG_MN', '_NEG_1', '_ZERO', '_NEG1_long']:\n",
    "            if hasattr(self, attr):\n",
    "                delattr(self, attr)\n",
    "\n",
    "        flush_gpu(dev)\n",
    "\n",
    "    def __del__(self):\n",
    "        try:\n",
    "            self.cleanup()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    def _print_gpu_mem(self, tag: str = \"\"):\n",
    "        \"\"\"CUDA 메모리 사용량 출력 (디버깅용).\"\"\"\n",
    "        if self.device.type == \"cuda\":\n",
    "            alloc = torch.cuda.memory_allocated(self.device) / (1024 ** 2)\n",
    "            reserved = torch.cuda.memory_reserved(self.device) / (1024 ** 2)\n",
    "            print(f\"  [GPU mem {tag}] alloc={alloc:.1f}MB, reserved={reserved:.1f}MB\")\n",
    "\n",
    "    # =================================================================\n",
    "    #                         Public Interface\n",
    "    # =================================================================\n",
    "    def run(self) -> Tuple[List[int], float]:\n",
    "        best_route, best_cost = None, None\n",
    "        stable = 0\n",
    "        last_cost = None\n",
    "\n",
    "        for it in range(self.iters):\n",
    "            phi_t, eta_t, rho_t = self._derive_bm_messages()\n",
    "            psi, alpha, backptr = self._forward(rho_t)\n",
    "            beta, xi = self._backward(rho_t)\n",
    "            delta_t = self._compute_delta(psi, beta, rho_t)\n",
    "\n",
    "            # γ̃, ω̃ damping 업데이트\n",
    "            gamma_new = eta_t + delta_t\n",
    "            omega_new = phi_t + delta_t\n",
    "            self.gamma_t = self.damping * gamma_new + (1 - self.damping) * self.gamma_t\n",
    "            self.omega_t = self.damping * omega_new + (1 - self.damping) * self.omega_t\n",
    "            print(self.gamma_t[0])\n",
    "            route = self._decode(alpha, backptr)\n",
    "            cost = self._route_cost(route)\n",
    "\n",
    "            # ── 중간 텐서 즉시 해제 ──\n",
    "            del psi, beta, xi, phi_t, eta_t, rho_t, delta_t\n",
    "            del gamma_new, omega_new\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"[{it+1:03d}] cost={cost:.6f}  route={route}\")\n",
    "\n",
    "            if best_cost is None or cost < best_cost:\n",
    "                best_cost, best_route = cost, route\n",
    "\n",
    "            # decode에서 사용 완료\n",
    "            del alpha, backptr\n",
    "\n",
    "            if last_cost is not None and abs(cost - last_cost) <= self.cost_tol:\n",
    "                stable += 1\n",
    "            else:\n",
    "                stable = 0\n",
    "            last_cost = cost\n",
    "            if stable >= self.patience:\n",
    "                break\n",
    "\n",
    "        return best_route, best_cost\n",
    "\n",
    "    # =================================================================\n",
    "    #        Bipartite Matching: φ̃, η̃, ρ̃ (eq 37, 40, 41)\n",
    "    # =================================================================\n",
    "    def _derive_bm_messages(self):\n",
    "        \"\"\"\n",
    "        벡터화된 top-2 기반 \"자기 제외 max\" 계산.\n",
    "        \"\"\"\n",
    "        N, T = self.N, self.T\n",
    "        dev, dt = self.device, self.dtype\n",
    "\n",
    "        # --- eq (40): φ̃_it = -max_{i'≠i} γ̃_{i't} ---\n",
    "        phi_t = torch.zeros((N, T), dtype=dt, device=dev)\n",
    "        if N > 1:\n",
    "            top2_v, top2_i = self.gamma_t.topk(2, dim=0)  # [2, T]\n",
    "            for i in range(N):\n",
    "                is_top = (top2_i[0] == i)\n",
    "                phi_t[i] = -torch.where(is_top, top2_v[1], top2_v[0])\n",
    "\n",
    "        # --- eq (37): η̃_it = -max_{t'≠t} ω̃_{it'} ---\n",
    "        eta_t = torch.zeros((N, T), dtype=dt, device=dev)\n",
    "        if T > 1:\n",
    "            top2_v, top2_i = self.omega_t.topk(2, dim=1)  # [N, 2]\n",
    "            for t in range(T):\n",
    "                is_top = (top2_i[:, 0] == t)\n",
    "                eta_t[:, t] = -torch.where(is_top, top2_v[:, 1], top2_v[:, 0])\n",
    "\n",
    "        rho_t = eta_t + phi_t\n",
    "        return phi_t, eta_t, rho_t\n",
    "\n",
    "    # =================================================================\n",
    "    #         Forward Pass (벡터화): ψ_t, α_t (eq 15-16)\n",
    "    # =================================================================\n",
    "    def _forward(self, rho_t):\n",
    "        \"\"\"\n",
    "        per-city 루프 + 배치 gather로 마스크 루프 제거.\n",
    "        메모리: O(M·N) per time step.\n",
    "        \"\"\"\n",
    "        T, N, M = self.T, self.N, self.M\n",
    "        dev, dt = self.device, self.dtype\n",
    "        depot = self.depot\n",
    "\n",
    "        psi     = torch.full((T + 1, M, N), NEG, dtype=dt, device=dev)\n",
    "        alpha   = torch.full((T + 1, M, N), NEG, dtype=dt, device=dev)\n",
    "        backptr = torch.full((T + 1, M, N), -1, dtype=torch.long, device=dev)\n",
    "\n",
    "        # λ_sum 전체 사전 계산: [N, T]\n",
    "        lambda_sum_all = rho_t - rho_t.mean(dim=0, keepdim=True)\n",
    "\n",
    "        # ── t = 1: depot → 첫 도시 ──\n",
    "        for a in range(N):\n",
    "            m = 1 << a\n",
    "            s_val = self.s[depot, a]\n",
    "            psi[1, m, a] = s_val\n",
    "            alpha[1, m, a] = s_val + lambda_sum_all[a, 0]\n",
    "\n",
    "        # ── t = 2 .. T: 배치 처리 ──\n",
    "        for t in range(2, T + 1):\n",
    "            t_idx = t - 1\n",
    "            valid_pop = (self._popcount == t)  # [M]\n",
    "\n",
    "            for a in range(N):\n",
    "                # valid[m] = popcount(m)==t AND bit a set in m\n",
    "                valid = valid_pop & self._bit_set[:, a]  # [M]\n",
    "\n",
    "                # prev_mask[m] = m ^ (1<<a)\n",
    "                prev = self._prev_mask[:, a]  # [M]\n",
    "\n",
    "                # gather alpha[t-1, prev[m], :] → [M, N]\n",
    "                alpha_gathered = alpha[t - 1][prev]  # [M, N]\n",
    "\n",
    "                # scores[m, last] = alpha[t-1, prev, last] + s(last, a)\n",
    "                scores = alpha_gathered + self.s_city[:, a].unsqueeze(0)  # [M, N]\n",
    "\n",
    "                # last는 prev_mask에 bit가 있어야 유효\n",
    "                valid_last = self._bit_set[prev]  # [M, N]\n",
    "                scores = torch.where(valid_last, scores, self._NEG_MN)\n",
    "\n",
    "                # max over last\n",
    "                psi_val, argmax_last = scores.max(dim=1)  # [M]\n",
    "\n",
    "                psi[t, :, a] = torch.where(valid, psi_val, self._NEG_M)\n",
    "                alpha[t, :, a] = torch.where(\n",
    "                    valid,\n",
    "                    psi_val + lambda_sum_all[a, t_idx],\n",
    "                    self._NEG_M\n",
    "                )\n",
    "                backptr[t, :, a] = torch.where(\n",
    "                    valid, argmax_last, self._NEG1_long.expand(M)\n",
    "                )\n",
    "\n",
    "        return psi, alpha, backptr\n",
    "\n",
    "    # =================================================================\n",
    "    #         Backward Pass (벡터화): β_t, ξ_t (eq 17-18)\n",
    "    # =================================================================\n",
    "    def _backward(self, rho_t):\n",
    "        \"\"\"\n",
    "        per-city gather + 배치 scatter.\n",
    "        \"\"\"\n",
    "        T, N, M = self.T, self.N, self.M\n",
    "        dev, dt = self.device, self.dtype\n",
    "        full = (1 << N) - 1\n",
    "        depot = self.depot\n",
    "\n",
    "        beta = torch.full((T + 1, M, N), NEG, dtype=dt, device=dev)\n",
    "        xi   = torch.full((T + 1, M, N), NEG, dtype=dt, device=dev)\n",
    "\n",
    "        lambda_sum_all = rho_t - rho_t.mean(dim=0, keepdim=True)\n",
    "\n",
    "        # ── t = T: closure ──\n",
    "        for a in range(N):\n",
    "            beta[T, full, a] = self.s[a, depot]\n",
    "            xi[T, full, a] = self.s[a, depot] + lambda_sum_all[a, T - 1]\n",
    "\n",
    "        # ── t = T-1 .. 1 ──\n",
    "        for t in range(T - 1, 0, -1):\n",
    "            t_idx = t - 1\n",
    "            valid_pop = (self._popcount == t)  # [M]\n",
    "\n",
    "            # xi_next_a[m, a] = xi[t+1, m|(1<<a), a]\n",
    "            xi_next = xi[t + 1]  # [M, N]\n",
    "            xi_next_a = torch.full((M, N), NEG, dtype=dt, device=dev)\n",
    "            for a in range(N):\n",
    "                next_m = self._next_mask[:, a]  # [M]\n",
    "                xi_next_a[:, a] = xi_next[next_m, a]\n",
    "\n",
    "            for last in range(N):\n",
    "                valid = valid_pop & self._bit_set[:, last]  # [M]\n",
    "\n",
    "                # scores[m, a] = s(last, a) + xi[t+1, m|(1<<a), a]\n",
    "                scores = self.s_city[last, :].unsqueeze(0) + xi_next_a  # [M, N]\n",
    "\n",
    "                # a는 현재 마스크에 없어야 함\n",
    "                not_in_mask = ~self._bit_set  # [M, N]\n",
    "                scores = torch.where(not_in_mask, scores, self._NEG_MN)\n",
    "\n",
    "                # max over a\n",
    "                beta_val, _ = scores.max(dim=1)  # [M]\n",
    "\n",
    "                beta[t, :, last] = torch.where(valid, beta_val, self._NEG_M)\n",
    "                xi[t, :, last] = torch.where(\n",
    "                    valid,\n",
    "                    beta_val + lambda_sum_all[last, t_idx],\n",
    "                    self._NEG_M\n",
    "                )\n",
    "\n",
    "        return beta, xi\n",
    "\n",
    "    # =================================================================\n",
    "    #     δ̃ 계산 (최적화): peak 팩토리제이션 (eq 42)\n",
    "    # =================================================================\n",
    "    def _compute_delta(self, psi, beta, rho_t):\n",
    "        \"\"\"\n",
    "        핵심 최적화:\n",
    "          gamma_val = ψ + β  (경로 메트릭)\n",
    "          peak[t, a] = max_{m: popcount=t, bit_a∈m} gamma_val[t, m, a]\n",
    "\n",
    "        λ_sum_excl_i는 mask에 무관하므로 factoring 가능:\n",
    "          best_with[i,t]    = excl_λ(i,t,i) + peak[t,i]\n",
    "          best_without[i,t] = max_{a≠i} [excl_λ(i,t,a) + peak[t,a]]\n",
    "\n",
    "        복잡도: O(N²·T) vs 원래 O(N²·T·M)\n",
    "        \"\"\"\n",
    "        T, N, M = self.T, self.N, self.M\n",
    "        dev, dt = self.device, self.dtype\n",
    "\n",
    "        gamma_val = psi + beta  # [T+1, M, N]\n",
    "\n",
    "        # --- peak[t, a] 계산 ---\n",
    "        peak = torch.full((T + 1, N), NEG, dtype=dt, device=dev)\n",
    "        for t in range(1, T + 1):\n",
    "            valid = (self._popcount == t).unsqueeze(1) & self._bit_set  # [M, N]\n",
    "            gv = gamma_val[t]  # [M, N]\n",
    "            gv_masked = torch.where(valid & (gv > NEG / 2), gv, self._NEG_MN)\n",
    "            peak[t] = gv_masked.max(dim=0).values  # [N]\n",
    "\n",
    "        # --- lambda_sum_all ---\n",
    "        lambda_sum_all = rho_t - rho_t.mean(dim=0, keepdim=True)  # [N, T]\n",
    "\n",
    "        delta = torch.zeros((N, T), dtype=dt, device=dev)\n",
    "\n",
    "        for t in range(1, T + 1):\n",
    "            t_idx = t - 1\n",
    "            pk = peak[t]                    # [N]\n",
    "            lsa = lambda_sum_all[:, t_idx]  # [N]\n",
    "            rho_col = rho_t[:, t_idx]       # [N]\n",
    "\n",
    "            # --- best_with[i] ---\n",
    "            # excl_λ(i, t, a=i) = λ_sum(t,i) - (N-1)/N · ρ[i,t]\n",
    "            excl_w = lsa - (N - 1.0) / N * rho_col  # [N]\n",
    "            best_with = excl_w + pk  # [N]\n",
    "            best_with = torch.where(pk > NEG / 2, best_with,\n",
    "                                    torch.full_like(best_with, NEG))\n",
    "\n",
    "            # --- best_without[i] = max_{a≠i} [excl_λ(i,t,a) + peak[t,a]] ---\n",
    "            # excl_λ(i, t, a≠i) = λ_sum(t,a) + 1/N · ρ[i,t]\n",
    "            # score[i, a] = (lsa[a] + pk[a]) + 1/N · ρ[i,t]\n",
    "            base_a = lsa + pk  # [N]\n",
    "            rho_term = (1.0 / N) * rho_col  # [N]\n",
    "\n",
    "            scores_wo = base_a.unsqueeze(0) + rho_term.unsqueeze(1)  # [N, N]\n",
    "            scores_wo.fill_diagonal_(NEG)\n",
    "\n",
    "            # peak가 NEG인 a 제외\n",
    "            invalid_pk = (pk <= NEG / 2)\n",
    "            scores_wo[:, invalid_pk] = NEG\n",
    "\n",
    "            best_without = scores_wo.max(dim=1).values  # [N]\n",
    "\n",
    "            both_neg = (best_with <= NEG / 2) & (best_without <= NEG / 2)\n",
    "            delta[:, t_idx] = torch.where(both_neg, self._ZERO,\n",
    "                                          best_with - best_without)\n",
    "\n",
    "        return delta\n",
    "\n",
    "    # =================================================================\n",
    "    #                         Route Decoding\n",
    "    # =================================================================\n",
    "    def _decode(self, alpha, backptr):\n",
    "        T, N = self.T, self.N\n",
    "        full = (1 << N) - 1\n",
    "        depot = self.depot\n",
    "\n",
    "        final_scores = alpha[T, full, :] + self.s[:N, depot]\n",
    "        best_last = final_scores.argmax().item()\n",
    "\n",
    "        if final_scores[best_last].item() <= NEG / 2:\n",
    "            return self._greedy_fallback()\n",
    "\n",
    "        route_inner = []\n",
    "        mask = full\n",
    "        a = best_last\n",
    "        for t in range(T, 0, -1):\n",
    "            route_inner.append(a)\n",
    "            prev_a = backptr[t, mask, a].item()\n",
    "            mask = mask ^ (1 << a)\n",
    "            a = prev_a\n",
    "        route_inner.reverse()\n",
    "        route = [depot] + route_inner + [depot]\n",
    "        return [int(self.inv_perm[c]) for c in route]\n",
    "\n",
    "    def _greedy_fallback(self):\n",
    "        N, depot = self.N, self.depot\n",
    "        route = [depot]\n",
    "        used = set()\n",
    "        for _ in range(N):\n",
    "            scores = self.s[route[-1], :N].clone()\n",
    "            for u in used:\n",
    "                scores[u] = NEG\n",
    "            a = scores.argmax().item()\n",
    "            used.add(a)\n",
    "            route.append(a)\n",
    "        route.append(depot)\n",
    "        return [int(self.inv_perm[c]) for c in route]\n",
    "\n",
    "    def _route_cost(self, route) -> float:\n",
    "        cost = 0.0\n",
    "        for k in range(len(route) - 1):\n",
    "            cost += self.orig_D[route[k], route[k + 1]].item()\n",
    "        return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "13ad24fc-1ff1-439c-a5cc-7ac6aaab4948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import permutations\n",
    "\n",
    "class TSPBruteForceSolver:\n",
    "    \"\"\"\n",
    "    Brute-force TSP solver for small N.\n",
    "    - Fixes start_city to remove rotational symmetry.\n",
    "    - Returns a Hamiltonian cycle: [start, ..., start]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, D: np.ndarray, start_city: int = 0, verbose: bool = False):\n",
    "        D = np.asarray(D, dtype=float)\n",
    "        if D.ndim != 2 or D.shape[0] != D.shape[1]:\n",
    "            raise ValueError(\"D must be a square 2D matrix.\")\n",
    "        if not (0 <= start_city < D.shape[0]):\n",
    "            raise ValueError(\"start_city out of range.\")\n",
    "        self.D = D\n",
    "        self.N = D.shape[0]\n",
    "        self.start = start_city\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def route_cost(self, route):\n",
    "        \"\"\"\n",
    "        route: list/tuple of cities, must be a cycle [s, ..., s]\n",
    "        \"\"\"\n",
    "        total = 0.0\n",
    "        for a, b in zip(route[:-1], route[1:]):\n",
    "            total += float(self.D[a, b])\n",
    "        return total\n",
    "\n",
    "    def run(self):\n",
    "        cities = list(range(self.N))\n",
    "        others = [c for c in cities if c != self.start]\n",
    "\n",
    "        best_cost = float(\"inf\")\n",
    "        best_route = None\n",
    "\n",
    "        # Evaluate all permutations of the remaining cities\n",
    "        # Tour: start -> perm... -> start\n",
    "        for idx, perm in enumerate(permutations(others), start=1):\n",
    "            route = (self.start,) + perm + (self.start,)\n",
    "            cost = 0.0\n",
    "            # inline cost to reduce overhead a bit\n",
    "            for a, b in zip(route[:-1], route[1:]):\n",
    "                cost += self.D[a, b]\n",
    "\n",
    "            if cost < best_cost:\n",
    "                best_cost = float(cost)\n",
    "                best_route = list(route)\n",
    "                if self.verbose:\n",
    "                    print(f\"[BruteForce] New best @ {idx}: cost={best_cost:.6f}, route={best_route}\")\n",
    "\n",
    "        return best_route, best_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c60ee5f9-b41c-4e7e-b706-7b195f39f3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Device: mps] N=11, M=2048, peak mem ~9 MB\n",
      "tensor([-0.2678, -0.0016, -0.0502, -0.1002, -0.0227, -0.0147,  0.0016, -0.0560,\n",
      "        -0.0836, -0.0522, -0.0023], device='mps:0')\n",
      "[001] cost=1.356860  route=[0, 10, 3, 4, 8, 2, 5, 11, 9, 1, 7, 6, 0]\n",
      "tensor([-0.4774, -0.0194, -0.1075, -0.1888, -0.0496, -0.0397,  0.0194, -0.1074,\n",
      "        -0.1547, -0.1089, -0.0259], device='mps:0')\n",
      "[002] cost=1.356860  route=[0, 10, 3, 4, 8, 2, 5, 11, 9, 1, 7, 6, 0]\n",
      "tensor([-0.7837, -0.2186, -0.3142, -0.4069, -0.1912, -0.2227,  0.1700, -0.2587,\n",
      "        -0.3412, -0.2851, -0.2413], device='mps:0')\n",
      "[003] cost=1.356860  route=[0, 10, 3, 4, 8, 2, 5, 11, 9, 1, 7, 6, 0]\n",
      "tensor([-1.4784, -0.8331, -0.9663, -0.9561, -1.1899, -1.1377,  0.7708, -0.9471,\n",
      "        -0.9697, -0.9778, -0.8184], device='mps:0')\n",
      "[004] cost=1.356860  route=[0, 10, 3, 4, 8, 2, 5, 11, 9, 1, 7, 6, 0]\n",
      "tensor([-3.2056, -2.4787, -2.6381, -2.5736, -3.3609, -3.0519,  2.3544, -2.6702,\n",
      "        -2.6562, -2.6610, -2.4220], device='mps:0')\n",
      "[005] cost=1.356860  route=[0, 10, 3, 4, 8, 2, 5, 11, 9, 1, 7, 6, 0]\n",
      "tensor([-7.6644, -6.8833, -7.0653, -6.9867, -8.3042, -7.6744,  6.6021, -7.2128,\n",
      "        -7.1234, -7.0286, -6.7347], device='mps:0')\n",
      "[006] cost=1.356860  route=[0, 10, 3, 4, 8, 2, 5, 11, 9, 1, 7, 6, 0]\n",
      "tensor([-19.6568, -18.8391, -19.0432, -18.9606, -20.9049, -19.7520,  18.2006,\n",
      "        -19.4098, -19.2538, -18.7851, -18.4529], device='mps:0')\n",
      "[007] cost=1.356860  route=[0, 10, 3, 4, 8, 2, 5, 11, 9, 1, 7, 6, 0]\n",
      "tensor([-52.5347, -51.6490, -51.9196, -51.8300, -54.4538, -52.4325,  50.1996,\n",
      "        -52.5482, -52.2353, -51.0139, -50.6551], device='mps:0')\n",
      "[008] cost=1.356860  route=[0, 10, 3, 4, 8, 2, 5, 11, 9, 1, 7, 6, 0]\n",
      "tensor([-143.3040, -141.8946, -142.6408, -142.6334, -145.6878, -142.2961,\n",
      "         139.0327, -143.2300, -142.6791, -140.1991, -139.8216], device='mps:0')\n",
      "[009] cost=1.356860  route=[0, 10, 3, 4, 8, 2, 5, 11, 9, 1, 7, 6, 0]\n",
      "tensor([-393.9661, -391.6960, -393.2171, -393.3685, -396.9832, -391.4120,\n",
      "         386.5419, -393.7173, -392.7925, -388.2606, -387.8701], device='mps:0')\n",
      "[010] cost=1.356860  route=[0, 10, 3, 4, 8, 2, 5, 11, 9, 1, 7, 6, 0]\n",
      "tensor([-1090.1082, -1086.4536, -1089.2238, -1089.6505, -1094.0702, -1085.0221,\n",
      "         1077.6104, -1089.5654, -1088.0459, -1080.2048, -1079.8055],\n",
      "       device='mps:0')\n",
      "[011] cost=1.356860  route=[0, 10, 3, 4, 8, 2, 5, 11, 9, 1, 7, 6, 0]\n",
      "tensor([-3030.1055, -3024.2322, -3029.0083, -3029.8906, -3035.5332, -3020.9294,\n",
      "         3009.4714, -3029.0835, -3026.6157, -3013.4595, -3013.0547],\n",
      "       device='mps:0')\n",
      "[012] cost=1.356860  route=[0, 10, 3, 4, 8, 2, 5, 11, 9, 1, 7, 6, 0]\n",
      "tensor([-8447.3330, -8437.9082, -8445.8984, -8447.5195, -8455.0723, -8431.5889,\n",
      "         8413.6680, -8445.5439, -8441.5547, -8419.8906, -8419.4834],\n",
      "       device='mps:0')\n",
      "[013] cost=1.356860  route=[0, 10, 3, 4, 8, 2, 5, 11, 9, 1, 7, 6, 0]\n",
      "tensor([-23591.9219, -23576.8125, -23589.9531, -23592.7559, -23603.3496,\n",
      "        -23565.6602,  23537.4102, -23588.8887, -23582.4785, -23547.1855,\n",
      "        -23546.7793], device='mps:0')\n",
      "[014] cost=1.356860  route=[0, 10, 3, 4, 8, 2, 5, 11, 9, 1, 7, 6, 0]\n",
      "tensor([-65958.9141, -65934.7266, -65956.0938, -65960.8125, -65976.2500,\n",
      "        -65915.8125,  65871.0312, -65953.9219, -65943.6250, -65886.5156,\n",
      "        -65886.1016], device='mps:0')\n",
      "[015] cost=1.356860  route=[0, 10, 3, 4, 8, 2, 5, 11, 9, 1, 7, 6, 0]\n",
      "tensor([-184525.8438, -184487.1094, -184521.7812, -184529.5000, -184552.6094,\n",
      "        -184455.8594,  184384.6250, -184517.7969, -184501.2031, -184409.2500,\n",
      "        -184408.8125], device='mps:0')\n",
      "[016] cost=1.356860  route=[0, 10, 3, 4, 8, 2, 5, 11, 9, 1, 7, 6, 0]\n",
      "tensor([-516416.5312, -516354.3125, -516410.2500, -516422.8438, -516458.0625,\n",
      "        -516303.5000,  516189.8750, -516403.2500, -516376.7812, -516229.0312,\n",
      "        -516228.5625], device='mps:0')\n",
      "[017] cost=1.356860  route=[0, 10, 3, 4, 8, 2, 5, 11, 9, 1, 7, 6, 0]\n",
      "tensor([-1445554.2500, -1445456.0000, -1445546.1250, -1445565.8750,\n",
      "        -1445621.6250, -1445372.5000,  1445191.5000, -1445534.7500,\n",
      "        -1445492.2500, -1445255.5000, -1445254.6250], device='mps:0')\n",
      "[018] cost=1.356860  route=[0, 10, 3, 4, 8, 2, 5, 11, 9, 1, 7, 6, 0]\n",
      "tensor([-4046895.7500, -4046737.0000, -4046881.5000, -4046914.5000,\n",
      "        -4047000.5000, -4046604.0000,  4046313.2500, -4046860.2500,\n",
      "        -4046797.0000, -4046415.2500, -4046414.7500], device='mps:0')\n",
      "[019] cost=1.356860  route=[0, 10, 3, 4, 8, 2, 5, 11, 9, 1, 7, 6, 0]\n",
      "tensor([-11330255., -11330000., -11330226., -11330283., -11330420., -11329783.,\n",
      "         11329323., -11330197., -11330095., -11329483., -11329482.],\n",
      "       device='mps:0')\n",
      "[020] cost=1.356860  route=[0, 10, 3, 4, 8, 2, 5, 11, 9, 1, 7, 6, 0]\n",
      "tensor([-31723014., -31722636., -31722976., -31723064., -31723294., -31722262.,\n",
      "         31721540., -31722936., -31722768., -31721802., -31721794.],\n",
      "       device='mps:0')\n",
      "[021] cost=1.356860  route=[0, 10, 3, 4, 8, 2, 5, 11, 9, 1, 7, 6, 0]\n",
      "\n",
      "Final Route: [0, 10, 3, 4, 8, 2, 5, 11, 9, 1, 7, 6, 0]\n",
      "Final Cost:  1.356860\n",
      "Total Time:  938.304 ms\n",
      "Cleanup done.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    N_CITIES = 12\n",
    "    D = np.random.rand(N_CITIES, N_CITIES)\n",
    "    np.fill_diagonal(D, 0)\n",
    "\n",
    "    '''start1 = time.time()\n",
    "    solver = TSPFactorGraphSolver(\n",
    "        D, start_city=0,\n",
    "        damping=0.3,\n",
    "        iters=100,\n",
    "        verbose=True,\n",
    "        patience=20,\n",
    "    )\n",
    "    route_fg, cost_fg = solver.run()\n",
    "    end1 = time.time()\n",
    "    print(f\"\\nFinal Route: {route_fg}\")\n",
    "    print(f\"Final Cost:  {cost_fg:.6f}\")\n",
    "    print(f\"Total Time:  {(end1 - start1)*1000:.3f} ms\\n\")'''\n",
    "\n",
    "    start2 = time.time()\n",
    "    solver_GPU = TSPFactorGraphSolverGPU(\n",
    "        D, start_city=0,\n",
    "        damping=0.3,\n",
    "        iters=100,\n",
    "        verbose=True,\n",
    "        patience=20,\n",
    "    )\n",
    "    route_GPU, cost_GPU = solver_GPU.run()\n",
    "    end2 = time.time()\n",
    "    \n",
    "    print(f\"\\nFinal Route: {route_GPU}\")\n",
    "    print(f\"Final Cost:  {cost_GPU:.6f}\")\n",
    "    print(f\"Total Time:  {(end2 - start2)*1000:.3f} ms\")\n",
    "\n",
    "    # 메모리 진단\n",
    "    if solver_GPU.device.type == \"cuda\":\n",
    "        solver_GPU._print_gpu_mem(\"before cleanup\")\n",
    "\n",
    "    solver_GPU.cleanup()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        alloc = torch.cuda.memory_allocated() / (1024 ** 2)\n",
    "        reserved = torch.cuda.memory_reserved() / (1024 ** 2)\n",
    "        print(f\"  [GPU mem after cleanup] alloc={alloc:.1f}MB, reserved={reserved:.1f}MB\")\n",
    "        print(f\"  ※ reserved는 PyTorch 풀이 유지하는 메모리. \"\n",
    "              f\"프로세스 종료 시 반환됨.\")\n",
    "    print(\"Cleanup done.\")\n",
    "\n",
    "    '''# 브루트포스 솔버 (정답)\n",
    "    start3 = time.time()\n",
    "    bf = TSPBruteForceSolver(D, start_city=0, verbose=False)\n",
    "    route_bf, cost_bf = bf.run()\n",
    "    end3 = time.time()\n",
    "    print(f\"\\n[BruteForce] Optimal Route: {route_bf}\")\n",
    "    print(f\"[BruteForce] Optimal Cost:  {cost_bf:.6f}\")\n",
    "    print(f\"Total Time:  {(end3 - start3)*1000:.3f} ms\")\n",
    "\n",
    "    # 비교 지표\n",
    "    gap = cost_fg - cost_bf\n",
    "    rel_gap = gap / (abs(cost_bf) + 1e-12)\n",
    "    print(f\"\\nGap (FG - BF):     {gap:.6f}\")\n",
    "    print(f\"Relative gap:      {rel_gap*100:.3f}%\")\n",
    "    print(f\"FG matches optimum? {abs(gap) < 1e-9}\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c65db7c-c397-4abe-aa41-4f3fa7572eed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
